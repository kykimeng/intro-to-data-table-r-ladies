---
title: "Introduction to `data.table`"
author: "Haema Nilakanta & Kim Ky"
date: "10/10/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

<style type="text/css">

body{ /* Normal  */
      font-size: 14px;
  }
td {  /* Table  */
  font-size: 10px;
}
h1 { /* Header 1 */
  font-size: 26px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 14px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

___

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(ggplot2)
```

# Overview

### What is `data.table`?

"`data.table` is an R package that provides an enhanced version of `data.frame`s." It provides a faster and more efficient way to do data manipulation while drastically reducing the amount of memory required. It's an excellent addition to your R venacular, particularly if you work with massive datasets. 

### Install and load

```{r,eval=F}
# install 
install.packages("data.table")
# load 
library(data.table)
```

# The Basics

```{r,echo=F, fig.width=10, fig.height=5}
ggplot() + 
  geom_text(aes(x = 0, y = 0, label = "DT[i, j, by]"), size = 16) +
  theme_classic() +
  theme(axis.line = element_blank(), axis.title = element_blank(), 
        axis.text = element_blank(), axis.ticks = element_blank()) +
  geom_line(aes(x = c(-2, -.3), y = c(-2, -0.25)), arrow = arrow(), size = 1) +
  geom_line(aes(x = c(0.15, 0.15), y = c(-2, -0.25)), arrow = arrow(), size = 1) +
  geom_line(aes(x = c(.9, 2), y = c(-.25, -2)), arrow = arrow(ends = "first"), size = 1) +
  geom_label(aes(x = c(-2.5, 0.15, 3), y = -2, label = c("WHERE\n(which rows)", "SELECT\n(do what)", "GROUP BY\n(group by what)")), size = 10) +
  scale_x_continuous(limits = c(-4, 5)) +
  scale_y_continuous(limits = c(-2.2, .5))
```

A lot of `data.table` commands are similar to the usual R `data.frame`. For example, you can create a data.table similar to how you create a new `data.frame`. 

```{r}
# create a data.table
DT <- data.table(x = c(1, 2, 3), y = c('a', 'b', 'c'))

# turn iris data.frame to a data.table object
data("iris")
iris_dt <- as.data.table(iris)
# setDT(iris) # turn iris into a data.table instead of making a copy
# print
iris_dt
```

When a `data.table` has more than 100 rows, it automatically prints the first five and the last five rows. 

### Subset

Select first 3 rows:

```{r}
# data.frame
iris[1:3,]
# data.table
iris_dt[1:3]
```

Find rows where `Species == "setosa"`:

```{r,eval=F}
# data.frame
iris[iris$Species == "setosa",]
# data.table
iris_dt[Species == "setosa"]
```

Find rows where `Species == "setosa"` and `Sepal.Length == 5`:

```{r,eval=F}
# data.frame
iris[iris$Species == "setosa" & iris$Sepal.Length == 5,]
# data.table
iris_dt[Species == "setosa" & Sepal.Length == 5]
# advanced data.table
tmp <- data.table(Species = "setosa", Sepal.Length = 5, key = c("Species", "Sepal.Length"))
setkey(iris_dt, Species, Sepal.Length)
iris_dt[tmp]
```

Notice with the usual `data.frame`, you end up calling the dataset multiple times, once to get the column information and every time a column is referenced. Meanwhile in the `data.table` package, you only need to refer to the dataset once. 

### Exercise 1

a. Import `mpg` dataset: `data("mpg")`
b. Convert `mpg` to a `data.table` object
c. Find rows where *manufacturer* is *audi*, *year* is *2008*, and *class* is *compact*

### Manipulation on Columns and Group By

Select subset of columns:

```{r,eval=F}
# data.frame
iris[, c("Species", "Petal.Width", "Petal.Length")]
# data.table
iris_dt[, .(Species, Petal.Width, Petal.Length)]
```

Calculate the mean value of `Sepal.Length`:

```{r}
# data.frame
mean(iris$Sepal.Length)
# data.table
iris_dt[, mean(Sepal.Length)]
```

One major advantage of `data.table`, is it can aggregate data with the table setting. For example, instead of using the `aggregate` function to compute the mean `Sepal.Length` by `Species`, we can do this directly in the `iris_dt`. This saves us a good chunk of time especially when we have a lot of rows to take a function over. 

```{r}
# data.frame
aggregate(Sepal.Length ~ Species, iris, mean) # no easy way to do this in base R
# data.table
iris_dt[, mean(Sepal.Length), by = .(Species)]
```

The `data.table` syntax of `.()` basically calls on certain columns. 

Consider now we only want to calculate mean `Sepal.Length` by `Species` where `Sepal.Width >= 3`, instead of first having to subset the entire dataset and then running the function we can do this in one step and create a new variable called `mean_sepal_length`. 

```{r}
# data.frame
aggregate(Sepal.Length ~ Species, iris[iris$Sepal.Width >= 3,], mean)
# data.table
iris_dt[Sepal.Width >= 3, .(mean_sepal_length = mean(Sepal.Length)), by = .(Species)]
```

Another powerful tool of this package is the `.N` command. This is a counting tool that holds the number of observations in the current group. For example, if we want to calculate the number of rows:

```{r}
# data.frame
nrow(iris)
# data.table
iris_dt[, .N]
```

Or calculate number of unique values for `Species`:

```{r}
# data.frame
length(unique(iris$Species))
# data.table
iris_dt[, uniqueN(Species)]
```

Or calculate number of observations by `Species`:

```{r}
# data.frame
data.frame(table(iris$Species))
# data.table
iris_dt[, .N, Species]
```

Multiple operations in one go: calculate mean and number of operations by `Species` 

```{r}
# data.table
iris_dt[, .(mean_sepal_length = mean(Sepal.Length), no_obs = .N), by = .(Species)]
```

A similar command to `.N` is `.I` which holds the row numbers, and can be assigned to a variable.

```{r}
# return row numbers
iris_dt[, .I]
# assign row numbers to variable row_id
iris_dt[, row_id := .I]
head(iris_dt, 3)
```

### Exercise 2

Use `mpg` dataset from above

a. Calulate minimum engine displacement (*displ*) and median highway miles per gallon (*hwy*) by manufacturer and class
b. Repeat a. but for year 2008 only

### Keys

Once you get more comfortable with `data.table`, you can start to take advantage of its unique options. One way to really speed up your use of a large dataset is by setting the key(s). This allows the merging of multiple datasets or subsetting to go much faster. `data.table`s are also sorted by their keys. 

Sort by `Species` and `Sepal.Length`:

```{r,eval=F}
# data.frame
iris[with(iris, order(Species, Sepal.Length)), ]
# data.table
iris_dt[order(Species, Sepal.Length)] # does not store the ordering, unless assign to a variable name
setkey(iris_dt, Species, Sepal.Length) # keys are added; data.table is always sorted by its key columns
```

Calculate mean `Sepal.Width` by `Species` and order the results by the calculated means:

```{r,eval=F}
# data.frame
tmp_iris <- aggregate(Sepal.Width ~ Species, iris, mean)
tmp_iris[with(tmp_iris, order(Species)),]
# data.table
iris_dt[, .(mean_sepal_width = mean(Sepal.Width)), keyby = .(Species)]
```

If it is not necessary to keep the original ordering of the variable(s) in `by` statement, using `keyby` can speed up even more. And the resulting `data.table` is sorted by the variable(s) in the `keyby` statement. 

### Defining function `:=`

You can also define new variables in `data.table` without using unneccesary memory with the `:=` function. 

```{r}
# create a new variable called newIris
iris_dt[, newIris := Petal.Width/Petal.Length]
head(iris_dt, 3)

# create a new variable mean_petal_width by species
iris_dt[, mean_petal_width := mean(Petal.Width), .(Species)]
head(iris_dt, 3)
```

Or, you can create multiple variables all in one go

```{r,eval=F}
iris_dt[, `:=` (newIris = Petal.Width/Petal.Length, 
                row_id = .I)]
# or
iris_dt[, c("newIris", "row_id") := list(Petal.Width/Petal.Length, .I)]
```

We can also use `:=` to remove columns.

```{r}
# remove newIris column
iris_dt[, newIris := NULL]
```

```{r,eval=F}
# remove mean_petal_width and row_id 
iris_dt[, c("mean_petal_width", "row_id") := NULL]
# or
cols <- c("mean_petal_width", "row_id")
iris_dt[, (cols) := NULL]
```

```{r}
# or 
iris_dt[, `:=` (mean_petal_width = NULL, row_id = NULL)]
head(iris_dt, 3)
```

# More Advanced 

### Joins

Inner join two `data.table`s:

```{r}
dt1 <- data.table(x = c(1, 2, 3), y = c("a", "b", "c"), key = c("y"))
dt2 <- data.table(y = c("a", "c", "d", "e"), z = c(10, 20, 30, 40), key = c("y")) 
dt1[dt2, nomatch = 0]
```

```{r,eval=F}
# or
dt1[dt2, on = .(y), nomatch = 0] # equi: merge(dt1, dt2)
```

`on` needs to be specified if no keys are set, or if `dt1` and `dt2` do not have the exact same keys, or if you want to join the two `data.table`s using variables other than the keys.

Left/Right joins:

```{r}
# keep everything from dt2
dt1[dt2, on = .(y)] # equi: merge(dt1, dt2, all.y = TRUE)
# keep everything from dt1
dt2[dt1, on = .(y)] # equi: merge(dt1, dt2, all.x = TRUE)
```

Full joins:

```{r}
merge(dt1, dt2, all = TRUE)
```

# So, why should you use `data.table`?

* concise and consistent syntax
    + `DT[i, j, by]`
* speed
    + faster than base R and `dplyr`
* efficient memory usage
    + sub-assign by reference: `DT[x < 0, y := NA]`
    + aggregate while joining: `DT1[DT2, list(z=sum(z)), by = .EACHI]` (equivalent to `DT1[, .(z=sum(z)), keyby=.(x,y)][DT2]`)
* features
    + `fread`, `fwrite`, automatic indexing, rolling joins, non-equivalent joins, etc.

More detail, [here](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly).


# References

1. Matt Dowle and Arun Srinivasan (2018). `data.table`: Extension of `data.frame`. R
  package version 1.11.4. https://CRAN.R-project.org/package=data.table
2. https://github.com/Rdatatable/data.table/wiki
3. https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly
4. http://brooksandrew.github.io/simpleblog/articles/advanced-data-table/
5. https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf
6. https://stackoverflow.com/questions/20039335/what-is-the-purpose-of-setting-a-key-in-data-table